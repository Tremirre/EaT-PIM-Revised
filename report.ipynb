{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWSN - EaT-PIM with simplified ingredients and ingredient metadata\n",
    "\n",
    "Bartosz Stachowiak 148259<br>\n",
    "Andrzej Kajdasz 148273"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "After reading [EaT-PIM: Substituting Entities in Procedural Instructions using Flow Graphs and Embeddings (Sola S. Shirai, HyeongSik Kim)](https://dspace.rpi.edu/bitstream/handle/20.500.13015/6364/ISWC_EaT_PIM.pdf?sequence=1&isAllowed=y&fbclid=IwAR3RaVCT2_kb0T5NmVNs2ulIxjWkbDBu8T9wfUrIk7pSrjLcRQEeA6BqVkg) we noticed a room for improvement in the appraoch. We hypothesize that we could improve the results by incorporating other information besides the ingredients themselves to improve the quality of the suggestions.\n",
    "<br><br>\n",
    "The first and most important task for us is to incorporate ingredient metadata in substitute prediction. The proposed model utilizes only the contextual information about the ingredient from recipes, which is not always accurate (completely different ingredients might be prepared in similar ways but do not taste alike). To improve the quality of the prediction, we could incorporate the metadata about individual ingredients (e.g. taste, type) and make a prediction as a combination of the two sources. The plan for us is to describe the metadata in form of a feature matrix (one-hot encoded), combine similarity metric between the missing ingredient and its alternatives, combine the metric with paper's original approach using weights, find most appropriate replacements\n",
    "<br><br>\n",
    "The second proposition equally important is to simplify the ingredients to get more informative predictions. The authors seem to have used very granular distinction between individual ingredients, which sometimes give very unhelpful substitution propositions (e.g. pork => boneless pork). Simplifying the distinction and grouping very similar ingredients could help with more insightful predictions. To achieve such results we will prune any modifiers from existing ingredients (e.g. boneless pork), aggregate ingredients to their simplest form.\n",
    "\n",
    "We strongly believe that with these two changes, we will be able to improve upon the initial solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA\n",
    "\n",
    "### 2.1 Original Dataset\n",
    "\n",
    "Raw data are taken from [kaggle - Food.com Recipes and Interactions dataset](https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions).\n",
    "\n",
    "This dataset was created by scraping over 230k recipies from [Food.com](food.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import collections\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/RAW_recipes.csv')\n",
    "df.set_index('id', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. Column Description\n",
    "\n",
    "The dataset consists of recipies, each described by the following:\n",
    "- **name** - name of the recipe, string, lowercase \n",
    "- **id** - id of the recipe, int\n",
    "- **minutes** - time needed to prepare the recipe in minutes, int\n",
    "- **contributor_id** - id of the contributor, int\n",
    "- **submitted** - date of submission, string, format: YYYY-MM-DD\n",
    "- **tags** - tags of the recipe, list of lowercase strings\n",
    "- **nutrition** - nutrition information, list of 7 floats:\n",
    "  - **calories (#)**\n",
    "  - **total fat (PDV)**\n",
    "  - **sugar (PDV)**\n",
    "  - **sodium (PDV)**\n",
    "  - **protein (PDV)**\n",
    "  - **saturated fat (PDV)**\n",
    "  - **carbohydrates (PDV)**\n",
    "- **n_steps** - number of steps in the recipe, int\n",
    "- **steps** - steps of the recipe, list of strings\n",
    "- **description** - description of the recipe, string\n",
    "- **ingredients** - ingredients of the recipe, list of strings\n",
    "- **n_ingredients** - number of ingredients in the recipe, int\n",
    "\n",
    "The authors of the paper have used only the **ingredients** and **steps** columns, which we will also use in our approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_string_list(string_list: str) -> list[str]:\n",
    "    return [token[1:-1] for token in string_list[1:-1].replace(\" , \", \"; \").split(\", \")]\n",
    "\n",
    "ingredients = df.ingredients.apply(parse_string_list).tolist()\n",
    "steps = df.steps.apply(parse_string_list).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, each recipe has 9 ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.n_ingredients.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.n_ingredients.hist(bins=80)\n",
    "plt.title(\"Number of ingredients per recipe\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_counts = collections.Counter(itertools.chain.from_iterable(ingredients))\n",
    "\n",
    "print(\"Total number of ingredients:\", len(ingredient_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_ingredients = ingredient_counts.most_common(20)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh([ingredient for ingredient, _ in most_common_ingredients], [count for _, count in most_common_ingredients])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_arr = np.array(tuple(ingredient_counts.values()))\n",
    "used_only_once_count = (counts_arr < 2).sum()\n",
    "used_less_than_5_count = (counts_arr < 5).sum()\n",
    "\n",
    "print(f\"{used_only_once_count / len(ingredient_counts) * 100:.2f}% of ingredients are used only in one recipe\")\n",
    "print(f\"{used_less_than_5_count / len(ingredient_counts) * 100:.2f}% of ingredients are used in less than 5 recipes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ing_x = [i for i in range(1, counts_arr.max() + 1)]\n",
    "ing_y = [(counts_arr < i).sum() / counts_arr.shape[0] for i in ing_x]\n",
    "plt.plot(ing_x, ing_y)\n",
    "plt.xscale('log')\n",
    "plt.title(\"Cumulative distribution of ingredient counts\")\n",
    "plt.xlabel(\"Used in less than n recipes\")\n",
    "plt.ylabel(\"% of ingredients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many ingredients are very sparsly used, which is a problem for our approach. They are unlikely to be useful in the prediction, but some of them might be grouped to some more general ingredients. Some examples of such ingredients are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ing in list(ingredient_counts.keys())[-10:]:\n",
    "    print(ing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3. Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, each recipe has 10 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.n_steps.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.n_ingredients.hist(bins=80)\n",
    "plt.title(\"Number of steps per recipe\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a recipe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, step in enumerate(steps[0]):\n",
    "    print(f\"{i+1:>4}. {step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Taste Dataset\n",
    "\n",
    "As we will want to include ingredients metadata in the component substitutes prediction, we needed a dataset that will facilitate this information.\n",
    "\n",
    "For this reason we analyzed the [DANS - TASTE, FAT AND TEXTURE DATABASE - TASTE VALUES DUTCH FOODS](https://easy.dans.knaw.nl/ui/datasets/id/easy-dataset:195372/tab/2) dataset as it appears to be most promising, yet overall it's hard to find an extensive dataset with ingredients from the original problem, hence at later stages of the project we might resort to using a different dataset or even creating our own.\n",
    "\n",
    "\n",
    "The selected dataset was compiled with the most frequently consumed foods in the Netherlands and their taste intensity relative to other available foods.\n",
    "For the purposes of our program, some of the data is irrelevant and will be omitted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_df = pd.read_csv('data/ingredients_taste/Dutch_Foods.csv').fillna(0)\n",
    "ingredient_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Column Description\n",
    "\n",
    "The dataset consists of products described by the following features:\n",
    "- **Food_code** - Food code, as much as possible based on the nevocode\n",
    "- **Product_brand** - Product tested\n",
    "- **NEVO_code** - Corresponding NEVO code (0=no NEVO code)\n",
    "- **Product_description_NL** - Product description for the nevocode, in Dutch\n",
    "- **Product_description_EN** - Product description for the nevocode, in English\n",
    "- **Food_group_code** - Food group code\n",
    "- **Food_group_NL** - Food group in Dutch\n",
    "- **Food_group_EN** - Food group in English\n",
    "- **Date** - Date of profiling\n",
    "- **Serving_methods** - Standardized serving methods (temperature, with or without crust, etc.)\n",
    "- **Preparation_method** - Standardized preparation method for cooked foods\n",
    "- **Reference_control_foods** - Reference foods (=1) and control foods (=2)\n",
    "\n",
    "For each five basic tastes (sweet, sour, bitter, umami, salt) and also fat:\n",
    "- **no_taste** - Number of panellists for *taste*\n",
    "- **m_taste** - Mean taste intensity value for *taste*\n",
    "- **sd_taste** - Standard deviation for the mean taste *taste* intensity value\n",
    "- **se_taste**- Standard error for the mean taste *taste* intensity value\n",
    "\n",
    "For our program we need only *taste* data with product description and food group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_df = ingredient_df.drop(columns = ['Reference_control_foods', 'Food_code', 'Product_brand', 'NEVO_code', 'Product_description_NL','Food_group_code', 'Food_group_NL', 'Date', 'Serving_methods', 'Preparation_method', 'no_sweet', 'no_sour', 'no_bitter', 'no_umami', 'no_fat', 'no_salt'])\n",
    "value_taste_columns = ['m_sweet', 'm_sour', 'm_bitter', 'm_umami', 'm_fat', 'm_salt']\n",
    "print(f'There are {ingredient_df.shape[0]} different products')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Food groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(ingredient_df.groupby('Food_group_EN'))} different food groups in the dataset\")\n",
    "ingredient_df.groupby('Food_group_EN').count()['Product_description_EN'].sort_values(ascending=False).plot.barh(figsize = (15, 10))\n",
    "plt.title('Number of products per food group')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four groups have only one product. This gives a very big contrast when comparing to a group of almost 80 different products of vegetables and (non) alcoholic beverages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Taste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_df.filter(items = value_taste_columns).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each product was rated on a scale of 0 to 100 for each of the tastes. Most products have a high value of only one of the tastes (rarely two)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for taste in value_taste_columns:\n",
    "    product = ingredient_df.sort_values(taste).tail(1)[['Product_description_EN', taste]]\n",
    "    print(f\"The most {taste[2:]} product was {product['Product_description_EN'].values[0]} with value {int(product[taste].values[0])}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Summary\n",
    "\n",
    "The structure of the original dataset will for sure be useful in our approach, as the ingredients are given in a uniform way, which will allow us to group them into more general groups and simplify the problem.\n",
    "\n",
    "After the grouping we should be able to use at least part of the taste dataset to enrich the information about the ingredients.\n",
    "\n",
    "Our main concern is that that taste dataset is relatively small: only 627 products, whereas the original dataset consists of almost 15k - even after simplification, it's likely we'll have missing metadata for some ingredients.\n",
    "\n",
    "For this reason we might switch to a different dataset or even create our own if we find it necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation - Exploring Approaches\n",
    "\n",
    "As a part of our project, we need to recategorize the ingredients to their simplest form. We tried two approaches - AI-based and rule-based.\n",
    "We also discarded the ingredients that could not be recategorized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Ingredients categorization by GPT-3.5 turbo model\n",
    "\n",
    "To facilitate AI-based categorization we decided to use OpenAI's GPT-3.5 turbo model using their API.\n",
    "\n",
    "This solution allowed us to quickly obtained the results of decent quality.\n",
    "The drawback however is that this approach is not reproducible and each run of the script will yield slightly different results (even with fixed seed and temperature = 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1. Loading the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIZED_INGREDIENTS_PATH = \"./data/categorized.json\"\n",
    "BAD_CATEGORY = \"unknown\"\n",
    "\n",
    "with open(CATEGORIZED_INGREDIENTS_PATH) as f:\n",
    "    categorized_ingredients: dict = json.load(f)\n",
    "\n",
    "categorized_ingredients.pop(BAD_CATEGORY, None)\n",
    "categorized_ingredients = {k.lower(): v for k, v in categorized_ingredients.items() if v != BAD_CATEGORY}\n",
    "\n",
    "failed = [\n",
    "    ingredient\n",
    "    for ingredient in ingredient_counts.keys()\n",
    "    if ingredient not in categorized_ingredients\n",
    "]\n",
    "print(f\"Failed to categorize {len(failed)} ingredients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the way GPT-3.5 turbo model works, some ingredients were not processed by the model - either skipped or processed incorrectly. We decided to discard these ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counted_categories = collections.Counter(categorized_ingredients.values())\n",
    "most_common = counted_categories.most_common(20)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh([category for category, _ in most_common], [count for _, count in most_common])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. Loss of data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_categories = len(counted_categories)\n",
    "num_ingredients = len(ingredient_counts)\n",
    "\n",
    "print(\"Total number of categories:\", num_categories)\n",
    "print(\"Total number of ingredients:\", num_ingredients)\n",
    "print(f\"Reduction in number of ingredients to: {num_categories / num_ingredients * 100:.2f}% of original size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_recategorized_ingredients = [\n",
    "    categorized_ingredients[ingredient]\n",
    "    for ingredient in itertools.chain.from_iterable(ingredients)\n",
    "    if ingredient in categorized_ingredients\n",
    "]\n",
    "\n",
    "len(ai_recategorized_ingredients)\n",
    "print(f\"Retained {len(ai_recategorized_ingredients) / df['n_ingredients'].sum() * 100:.2f}% of ingredients usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3. Recategorization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_recounts = collections.Counter(ai_recategorized_ingredients)\n",
    "most_common_ingredients_recategorized = ingredient_recounts.most_common(20)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axs[0].barh([ingredient for ingredient, _ in most_common_ingredients], [count for _, count in most_common_ingredients])\n",
    "axs[0].set_title(\"Most common ingredients (original)\")\n",
    "axs[1].barh([ingredient for ingredient, count in most_common_ingredients_recategorized if count > 1000], [count for ingredient, count in most_common_ingredients_recategorized if count > 1000])\n",
    "axs[1].set_title(\"Most common ingredients (after recategorization)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_RECIPIES_COUNT = 5\n",
    "\n",
    "re_counts_arr = np.array(tuple(ingredient_recounts.values()))\n",
    "re_ing_x = [i for i in range(1, re_counts_arr.max() + 1)]\n",
    "re_ing_y = [(re_counts_arr < i).sum() / re_counts_arr.shape[0] for i in re_ing_x]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axs[0].plot(ing_x, ing_y, label=\"Original\")\n",
    "axs[0].plot(re_ing_x, re_ing_y, label=\"After recategorization\")\n",
    "axs[0].plot([MIN_RECIPIES_COUNT, MIN_RECIPIES_COUNT], [0, 1], label=\"Minimum count\", linestyle=\"--\", color=\"black\")\n",
    "axs[0].set_xscale('log')\n",
    "axs[0].set_title(\"Cumulative distribution of ingredient counts\")\n",
    "axs[0].set_xlabel(\"Used in less than n recipes\")\n",
    "axs[0].set_ylabel(\"% of ingredients\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(sorted(counts_arr), label=\"Original\")\n",
    "axs[1].plot(sorted(re_counts_arr), label=\"After recategorization\")\n",
    "axs[1].plot([0, counts_arr.shape[0]], [MIN_RECIPIES_COUNT, MIN_RECIPIES_COUNT], label=\"Minimum count\", linestyle=\"--\", color=\"black\")\n",
    "axs[1].set_title(\"Sorted ingredient counts\")\n",
    "axs[1].set_xlabel(\"Ingredient index\")\n",
    "axs[1].set_ylabel(\"Number of recipes\")\n",
    "axs[1].legend()\n",
    "axs[1].set_yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_min_count = [(v, count) for v, count in ingredient_counts.most_common() if count >= MIN_RECIPIES_COUNT]\n",
    "ai_parsed_min_recount = [(v, count) for v, count in ingredient_recounts.most_common() if count >= MIN_RECIPIES_COUNT]\n",
    "\n",
    "print(f\"Number of ingredients with at least {MIN_RECIPIES_COUNT} recipes:\")\n",
    "print(f\"Original: {len(parsed_min_count)}\")\n",
    "print(f\"After recategorization: {len(ai_parsed_min_recount)}\")\n",
    "print(f\"Reduction: {len(ai_parsed_min_recount) / len(parsed_min_count) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4. Summary\n",
    "\n",
    "Recategorization by GPT-3.5 turbo model was a good starting point, but it was not perfect. It managed to reduce the number of ingredients from 15k to 3k overall, and from 8k to 2k valid frequently used enough ones, but this is still a lot of ingredients to work with - much more than the number of entries in the taste dataset.\n",
    "\n",
    "With this approach it will be necessary to create our own dataset with ingredients and their metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Ingredients categorization by analytical method\n",
    "\n",
    "As an alternative to using advanced AI, we also decided to use the analytical method of grouping components based on the noun used in it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Prepare group with key words\n",
    "\n",
    "Ingredients as keywords grouped into several categories. Created based on the most frequent nouns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from report_utils import grouping\n",
    "grouping.groups.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. Splite complex ingredients\n",
    "\n",
    "In a dataset there are several ingredients that appear as a combination of two three or even four different elements in fact such ingredients can be separated into their subcomponents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ingredient in ingredient_counts:\n",
    "    if \" and \" in ingredient and \" with \" in ingredient:\n",
    "        print(ingredient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator_words = [\" and \", \" or \", \" with \", \" & \", \" in \"]\n",
    "formula = grouping.create_formula(separator_words)\n",
    "\n",
    "splited_ingredient = {}\n",
    "for ingredient in ingredient_counts:\n",
    "    for word in grouping.split_ingriedents(ingredient, formula):\n",
    "        if word in splited_ingredient.keys():\n",
    "            splited_ingredient[word] += 1\n",
    "        else:\n",
    "            splited_ingredient[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of ingredients before split: {len(ingredient_counts)}\")\n",
    "print(f\"Number of ingredients after split: {len(splited_ingredient)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3. Grouping by the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_map = grouping.group_ingredients(splited_ingredient.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4. Recategorization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_list = [[len(group.keys()), key_group] for key_group, group in ingredient_map.items()]\n",
    "group_list.sort(reverse=True)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh([name.split(\"_\")[0] for _, name in group_list], [counts for counts, _ in group_list])\n",
    "plt.title(\"Number of different products in each group\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list = []\n",
    "for product in ingredient_map.values():\n",
    "    for key, products in product.items():\n",
    "        product_list.append([len(products), key])\n",
    "product_list.sort(reverse=True)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh([name for _, name in product_list[1:25]], [counts for counts, _ in product_list[1:25]])\n",
    "plt.title(\"Most popular products\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.5 Uncategorized products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncategorized = len(ingredient_map[\"uncategorized\"][\"uncategorized\"])\n",
    "print(f\"Number of uncategorized ingredients: {uncategorized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invert_map = {\n",
    "    ingredient: key\n",
    "    for key, group in ingredient_map.items()\n",
    "    for _, ingredients in group.items()\n",
    "    for ingredient in ingredients\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invert_map = grouping.invert_grouping(ingredient_map)\n",
    "\n",
    "rule_recategorized_ingredients = [\n",
    "    invert_map[ingredient]\n",
    "    for ingredient in itertools.chain.from_iterable(ingredients)\n",
    "    if ingredient in invert_map and invert_map[ingredient] != \"uncategorized\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_rule_recounts = collections.Counter(rule_recategorized_ingredients)\n",
    "most_common_ingredients_rule_recategorized = ingredient_rule_recounts.most_common(20)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axs[0].barh([ingredient for ingredient, _ in most_common_ingredients], [count for _, count in most_common_ingredients])\n",
    "axs[0].set_title(\"Most common ingredients (original)\")\n",
    "axs[1].barh([ingredient for ingredient, count in most_common_ingredients_rule_recategorized if count > 1000], [count for ingredient, count in most_common_ingredients_recategorized if count > 1000])\n",
    "axs[1].set_title(\"Most common ingredients (after recategorization)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.6. Comparison with baseline and previous approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_re_counts_arr = np.array(tuple(ingredient_rule_recounts.values()))\n",
    "rule_re_ing_x = [i for i in range(1, rule_re_counts_arr.max() + 1)]\n",
    "rule_re_ing_y = [(rule_re_counts_arr < i).sum() / rule_re_counts_arr.shape[0] for i in rule_re_ing_x]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axs[0].plot(ing_x, ing_y, label=\"Original\")\n",
    "axs[0].plot(re_ing_x, re_ing_y, label=\"After recategorization (AI)\")\n",
    "axs[0].plot(rule_re_ing_x, rule_re_ing_y, label=\"After recategorization (rule based)\")\n",
    "axs[0].plot([MIN_RECIPIES_COUNT, MIN_RECIPIES_COUNT], [0, 1], label=\"Minimum count\", linestyle=\"--\", color=\"black\")\n",
    "axs[0].set_xscale('log')\n",
    "axs[0].set_title(\"Cumulative distribution of ingredient counts\")\n",
    "axs[0].set_xlabel(\"Used in less than n recipes\")\n",
    "axs[0].set_ylabel(\"% of ingredients\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(sorted(counts_arr), label=\"Original\")\n",
    "axs[1].plot(sorted(re_counts_arr), label=\"After recategorization (AI)\")\n",
    "axs[1].plot(sorted(rule_re_counts_arr), label=\"After recategorization (rule based)\")\n",
    "axs[1].plot([0, counts_arr.shape[0]], [MIN_RECIPIES_COUNT, MIN_RECIPIES_COUNT], label=\"Minimum count\", linestyle=\"--\", color=\"black\")\n",
    "axs[1].set_title(\"Sorted ingredient counts\")\n",
    "axs[1].set_xlabel(\"Ingredient index\")\n",
    "axs[1].set_ylabel(\"Number of recipes\")\n",
    "axs[1].legend()\n",
    "axs[1].set_yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numbr of ingredients after recategorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ingredient_rule_recounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_parsed_min_recount = [(v, count) for v, count in ingredient_rule_recounts.most_common() if count >= MIN_RECIPIES_COUNT]\n",
    "\n",
    "print(f\"Number of ingredients with at least {MIN_RECIPIES_COUNT} recipes:\")\n",
    "print(f\"Original: {len(parsed_min_count)}\")\n",
    "print(f\"After recategorization: {len(rule_parsed_min_recount)}\")\n",
    "print(f\"Reduction: {len(rule_parsed_min_recount) / len(parsed_min_count) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.7. Summary\n",
    "\n",
    "The second approach brings down the number of much more substantially, down to 250 ingredients. This is a much more manageable number, but it's still a lot more than the number of entries in the taste dataset.\n",
    "\n",
    "Moreover this method omits almost 1k ingredients, which is a lot of data to lose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Conclusion from data preparation\n",
    "\n",
    "Both approaches have their pros and cons. The AI-based approach is much more flexible and can be easily extended to include more categories, but it's not reproducible and the results are not always the same. The analytical approach is much more stable, but it's not as flexible and requires manual work to create the categories.\n",
    "\n",
    "Analyzing the results of AI approach more deeply we may still observe some redundancy, such as having separate categories for \"beef\" and \"roasted beef\" and having several ingredients split into their singular and plural form.\n",
    "\n",
    "Henceforth we decided to apply extended analytical method on the results of AI-based approach to get the best of both worlds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation - Combining both methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Applying analytical method on the results of AI-based approach\n",
    "\n",
    "As stated in the previous section, we decided to apply the analytical method on the results of AI-based approach to get the best of both worlds.\n",
    "\n",
    "Whilst this leads to less ingredients than the analytical method alone, the final result is of better quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_grouping = grouping.group_ingredients(categorized_ingredients.values())\n",
    "final_inversed_map = grouping.invert_grouping(final_grouping)\n",
    "\n",
    "len(final_grouping[\"uncategorized\"][\"uncategorized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counted_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ingredient in set(final_grouping['uncategorized']['uncategorized']):\n",
    "    print(f\"{ingredient:<20} {counted_categories[ingredient]:>4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rule_recategorized_ingredients = [\n",
    "    final_inversed_map[ingredient]\n",
    "    for ingredient in itertools.chain.from_iterable(ingredients)\n",
    "    if ingredient in final_inversed_map and final_inversed_map[ingredient] != \"uncategorized\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Recategorization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_final_recounts = collections.Counter(final_rule_recategorized_ingredients)\n",
    "\n",
    "final_re_counts_arr = np.array(tuple(ingredient_final_recounts.values()))\n",
    "\n",
    "final_ing_x = [i for i in range(1, final_re_counts_arr.max() + 1)]\n",
    "final_ing_y = [(final_re_counts_arr < i).sum() / final_re_counts_arr.shape[0] for i in final_ing_x]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axs[0].plot(ing_x, ing_y, label=\"Original\")\n",
    "axs[0].plot(re_ing_x, re_ing_y, label=\"After recategorization (AI)\")\n",
    "axs[0].plot(rule_re_ing_x, rule_re_ing_y, label=\"After recategorization (rule based)\")\n",
    "axs[0].plot(final_ing_x, final_ing_y, label=\"After recategorization (final)\")\n",
    "axs[0].plot([MIN_RECIPIES_COUNT, MIN_RECIPIES_COUNT], [0, 1], label=\"Minimum count\", linestyle=\"--\", color=\"black\")\n",
    "axs[0].set_xscale('log')\n",
    "axs[0].set_title(\"Cumulative distribution of ingredient counts\")\n",
    "axs[0].set_xlabel(\"Used in less than n recipes\")\n",
    "axs[0].set_ylabel(\"% of ingredients\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(sorted(counts_arr), label=\"Original\")\n",
    "axs[1].plot(sorted(re_counts_arr), label=\"After recategorization (AI)\")\n",
    "axs[1].plot(sorted(rule_re_counts_arr), label=\"After recategorization (rule based)\")\n",
    "axs[1].plot(sorted(final_re_counts_arr), label=\"After recategorization (final)\")\n",
    "axs[1].plot([0, counts_arr.shape[0]], [MIN_RECIPIES_COUNT, MIN_RECIPIES_COUNT], label=\"Minimum count\", linestyle=\"--\", color=\"black\")\n",
    "axs[1].set_title(\"Sorted ingredient counts\")\n",
    "axs[1].set_xlabel(\"Ingredient index\")\n",
    "axs[1].set_ylabel(\"Number of recipes\")\n",
    "axs[1].legend()\n",
    "axs[1].set_yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common ingredients recategorized\n",
    "\n",
    "most_common_ingredients_recategorized = ingredient_final_recounts.most_common(20)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axs[0].barh([ingredient for ingredient, _ in most_common_ingredients], [count for _, count in most_common_ingredients])\n",
    "axs[0].set_title(\"Most common ingredients (original)\")\n",
    "axs[1].barh([ingredient for ingredient, count in most_common_ingredients_recategorized if count > 1000], [count for ingredient, count in most_common_ingredients_recategorized if count > 1000])\n",
    "axs[1].set_title(\"Most common ingredients (after recategorization)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_reduction = {\n",
    "    \"original\": len(ingredient_counts),\n",
    "    \"ai\": len(ai_parsed_min_recount),\n",
    "    \"rule\": len(rule_parsed_min_recount),\n",
    "    \"final\": len(ingredient_final_recounts)\n",
    "}\n",
    "\n",
    "usage_reduction = {\n",
    "    \"original\": sum(ingredient_counts.values()),\n",
    "    \"ai\": sum(dict(ai_parsed_min_recount).values()),\n",
    "    \"rule\": sum(dict(rule_parsed_min_recount).values()),\n",
    "    \"final\": sum(ingredient_final_recounts.values())\n",
    "}\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axs[0].bar(categories_reduction.keys(), categories_reduction.values())\n",
    "axs[0].set_title(\"Number of ingredients after recategorization\")\n",
    "\n",
    "axs[1].bar(usage_reduction.keys(), usage_reduction.values())\n",
    "axs[1].set_title(\"Number of ingredient usages after recategorization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/final_recategorized.json\", 'w') as f:\n",
    "    json.dump(list(ingredient_final_recounts.keys()), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preparation - Ingredients metadata\n",
    "\n",
    "As a part of our project, we need to enrich the ingredients with metadata. We decided to create our own dataset with ingredients and their metadata, using once again AI-based approach.\n",
    "\n",
    "We tasked OpenAI's GPT-3.5 turbo model with generating the metadata for each ingredient, that contains the following information:\n",
    "- **origin** - origin of the ingredient, one of: \"animal\", \"plant\", \"other\"\n",
    "- **type** - type of the ingredient, one of: \"raw\", \"processed\"\n",
    "- **state** - state of the ingredient, one of: \"solid\", \"liquid\", \"gas\"\n",
    "- **texture** - texture of the ingredient, one of: \"soft\", \"hard\", \"crunchy\", \"smooth\"\n",
    "- **taste** - taste of the ingredient, one of: \"sweet\", \"sour\", \"bitter\", \"umami\", \"salty\"\n",
    "- **taste-intensity** - intensity of the taste of the ingredient, one of: \"low\", \"medium\", \"high\"\n",
    "- **smell** - smell of the ingredient, one of: \"sweet\", \"sour\", \"bitter\", \"umami\", \"salty\"\n",
    "- **smell-intensity** - intensity of the smell of the ingredient, one of: \"low\", \"medium\", \"high\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Loading the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/characterized.json\") as f:\n",
    "    characterized_ingredients_df = pd.read_json(f).T\n",
    "\n",
    "characterized_ingredients_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 4, figsize=(15, 5))\n",
    "for i, column in enumerate(characterized_ingredients_df.columns):\n",
    "    ax = axs[i // 4, i % 4]\n",
    "    characterized_ingredients_df[column].value_counts().plot.barh(ax=ax, title=column)\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eatpim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
